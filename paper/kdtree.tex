\documentclass{article}

\usepackage[a4paper,margin=1.15in,footskip=0.25in]{geometry}

\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{textpos}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\usepackage{verbatim}
\usepackage{textcomp}
\usepackage{varwidth}
\usepackage[linesnumbered,ruled]{algorithm2e}

% For displaying code
\usepackage{listings}

% Theorem
\newtheorem{theorem}{Theorem}

% lists
\usepackage{outlines}
\usepackage{enumitem}
\newenvironment{tight_enum}{
\begin{enumerate}[label=\alph*.]
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
}{\end{enumerate}}

% \subsubsubsection{}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\newcommand\simpleparagraph[1]{%
  \stepcounter{paragraph}\paragraph*{\theparagraph\quad{}#1}}

\usepackage{color}
\usepackage{xcolor}
\usepackage{mdframed}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% \usepackage{courier}
\lstset{%frame=tb,
language=C++,
aboveskip=3mm,
belowskip=3mm,
showstringspaces=false,
columns=flexible,
basicstyle={\small\ttfamily},
numbers=none,
numberstyle=\tiny\color{gray},
keywordstyle=\color{blue},
commentstyle=\color{dkgreen},
stringstyle=\color{mauve},
breaklines=true,
breakatwhitespace=true,
tabsize=3
}

%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%
%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%
%%%%%%%%% BEGIN DOCUMENT %%%%%%%%%

%% Used tt when referring to function

\begin{document}

\title{Parallel Orthogonal Recursive Bisection}
\author{Team Metropolis: \\
		Jamshid 'James' Farzidayeri, JJ Lay, and Graham West}
\date{COMS 7900, Capstone}

\maketitle

\begin{abstract}
In our first project, we implemented a parallel sorting algorithm which utilized a local gradient-type optimization search to equalize the amount of data across different compute nodes in order to achieve maximum efficiency. In this project, we applied this algorithm to the problem of parallel orthogonal recursive bisection (ORB), i.e., the construction of $k$-d trees. In order to do this, we had to heavily modify the sorting algorithm in several ways, including 1) turning it into a callable function, 2) letting the rank 0 head node perform work while still managing the tasks, 3) incorporating the use of different MPI communicators, and 4) altering the adaptive binning technique for better convergence. In this paper, we will discuss how our $k$-d tree algorithm works, how we solved the various issues plaguing parallel sort (mentioned above), and how we tested and validated our work. We conclude with a discussion of the major difficulties in completing this project and how these difficulties could be minimized in the future.
\end{abstract}


\tableofcontents


%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{Introduction}

% farzi
The purpose of this project was to create a parallel searching algorithm by expanding on a previous parallel sorting project. Given a very large set of data ($10^{10}$ points), we developed an orthogonal recursive bisection (ORB) algorithm which organizes the data into a $k$-d tree. The $k$-d tree includes information such as daughter and parent nodes, spacial indexing and ranges, etc. Additionally, the implementation also maximized the use of MPI, having multiple computing nodes performing the work. Due to the recursive nature of the problem as well as our implementation of it, we required both serial and parallel tree-building functions. Eventually a search using three radii and $2 \times 10^7$ points from an existing file were performed and the output saved to a file.

% farzi
\subsection{Workflow}
One of the major issues we encountered during the previous project was overwriting each others' GitHub submissions. Our resolution was to create a master branch and three sub branches. Each person was assigned a sub branch that they could modify as they pleased. However, a modification in the master branch required two or more team members' consent. This vastly reduced the amount of issues encountered during push/pull request.

Another adjustment we made for our development process was to use so-called ``extreme coding." In this context, only one team member actually writes/runs code while the other team members help plan, troubleshoot, debug, etc. We would meet in WPS 305 as opposed to the computer lab, due to the lack of noise, larger screens, and number of whiteboards. REgarding the coding itself, Graham was typically the coder with James watching and reviewing what was written and ran. JJ would usually have a second terminal open so that he could do small tests or refer to certain code sections (or Google search troublesome issues) when we had questions. We met on a regular basis, generally starting during our scheduled class period and extending through the lunch period. In summary, this technique allowed us to find errors quickly, debug quickly, avoid merge conflicts, and improved everyone's knowledge of how each component of the code operates.

Also, when the project was assigned, each team member prototyped their own version of a serial $k$-d tree in their preferred language. After a discussion on the merits of each, it was determined that Grahamâ€™s MATLAB implementation would be used as the guide for the project. 


\subsection{Variables and conventions}
%%% NOTE %%%
%%% NOTE %%%
% 1) be consistent with the indices:
% 2) say head node (not master node)
% 3) say worker (not node) whenever possible
%%% NOTE %%%
%%% NOTE %%%
Note that we distinguish between \textbf{tree} nodes and \textbf{compute} nodes to avoid confusion.

\begin{mdframed}[backgroundcolor=blue!20]
	Counts:
	\setlength\itemsep{0.1pt}
	\setlength\parskip{0.1pt}
	\begin{itemize}
		\setlength\itemsep{0.1pt}
		\setlength\parskip{0.1pt}
		\item $N$: number of nodes
		\item $M$: max number of allowed time steps
		\item $L$: number of lines to read per file
		\item $L_w$: number of lines on the $w$th worker
		\item $D$: total number of lines/data points
	\end{itemize}
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!20]
	Indices:
	\setlength\itemsep{0.1pt}
	\setlength\parskip{0.1pt}
	\begin{itemize}
		\setlength\itemsep{0.1pt}
		\setlength\parskip{0.1pt}
		\item $m = 0, \cdots, M$: the time step of the bin adaptation scheme (likely less than $M$)
		\item $n = 0, \cdots, W$: spans the nodes
		\item $i = 0, \cdots, N$: spans the bin edges/indices
		\item $j = 0, \cdots, N-1$: spans the bin counts (this will occasionally subscript binI/E as well)
		\item $\ell_w = 0, \cdots, L_n-1$: spans the lines on the $n$-th node
		\item $k = 0, \cdots, 3$: the data column being sorted
	\end{itemize}
\end{mdframed}

\begin{mdframed}[backgroundcolor=blue!20]
	Variables:
	\setlength\itemsep{0.1pt}
	\setlength\parskip{0.1pt}
	\begin{itemize}
		\setlength\itemsep{0.1pt}
		\setlength\parskip{0.1pt}
		\item $\textrm{data}^n_{4\ell+k}$: data point on $\ell$th line and $k$th column on the $n$-th node
		\item ${E}^m_j$: bin edges (0 indexed) at time step $m$
		\item ${I}^{n,m}_j$: bin indices on node $n$
		\item ${C}^{n,m}_j$: bin counts on node $n$ at time step $m$
		\item ${C}^m_j$: total bin counts on head node (sum of node C's) at time step $m$
	\end{itemize}
\end{mdframed}


%%%%%%%%%
%
% Section: Implementation
%
%%%%%%%%%

\section{Implementation}

\begin{center}
\includegraphics[width=0.95\textwidth]{images/flow.png}
\end{center}

%%%%%%%%%
%
% Subsection: Development
%
%%%%%%%%%

\subsection{Development}

The application was developed in a mixture of C and C++ on a Linux cluster. The compiler for the project was \texttt{g++} version 4.8.5-28 with the switch \texttt{-std=c++11} in order to utilize features introduced with that standard. The three most commonly used C++11 standards were placeholder type specifiers (i.e., \texttt{auto}), range-based \texttt{for} loops, and the \texttt{nullptr} constant.

The C interface to MPI was selected for its familiarity and general support. While there are object-oriented C++ libraries such as \texttt{Boost.MPI} the team elected to use the C version as they had experience with that environment and time to learn a new library was a limiting factor. Another concern was the installation of alternative libraries beyond Open MPI 2.1.1.

Debugging was performed using the tools \texttt{gdb} and \texttt{valgrind}. \texttt{gdb} was used primarily while developing the serial code for building the tree to identify the source of segmentation faults. \texttt{valgrind} was instrumental in locating memory leaks and illegal memory references during the parallel development. Code was executed with the command:

\begin{lstlisting}{language=bash}
mpirun -np 4 valgrind ./main > dump.txt 2> dump2.txt
\end{lstlisting}

One major challenge during the debugging process was that output was not displayed in chronological order making it difficult to understand the flow of the code. A solution we implemented was to print a numeric identifier during the execution at the beginning of each line. Each section of code was assigned a five digit numeric value, and the \texttt{mpi} rank was printed along with this (an additional output sorting technique was use of \texttt{sleep(myRank)}):

\begin{mdframed}[backgroundcolor=blue!20]
	\setlength\itemsep{0.1pt}
	\setlength\parskip{0.1pt}
	\begin{verbatim}
39300 : receiveFiles: Rank 93 received /home/hal2a/localstorage/public/
                              coms7900-data/datafile00122.txt as tag 0
39301 : receiveFiles: Rank 93 received DONE! as tag 1
35400 : receiveFiles: Rank 54 received /home/hal2a/localstorage/public/
                              coms7900-data/datafile00157.txt as tag 0
35401 : receiveFiles: Rank 54 received /home/hal2a/localstorage/public/
                              coms7900-data/datafile00023.txt as tag 1
35402 : receiveFiles: Rank 54 received DONE! as tag 2
30000 : receiveFiles: Rank 0 received /home/hal2a/localstorage/public/
                              coms7900-data/datafile00407.txt as tag 0
39901 : receiveFiles: Rank 99 received DONE! as tag 1
30001 : receiveFiles: Rank 0 received /home/hal2a/localstorage/public/
                             coms7900-data/datafile00361.txt as tag 1
30002 : receiveFiles: Rank 0 received DONE! as tag 2
30900 : receiveFiles: Rank 9 received /home/hal2a/localstorage/public/
                             coms7900-data/datafile00020.txt as tag 0
30901 : receiveFiles: Rank 9 received /home/hal2a/localstorage/public/
                             coms7900-data/datafile00046.txt as tag 1
30902 : receiveFiles: Rank 9 received DONE! as tag 2
45700 : receiveFiles: Rank 157 received /home/hal2a/localstorage/public/
                               coms7900-data/datafile00086.txt as tag 0
45701 : receiveFiles: Rank 157 received DONE! as tag 1
70000 : Rank 11 is calling buildTree
70000 : Rank 70000 : Rank 125 is calling buildTree
70000 : Rank 131 is calling buildTree	
    \end{verbatim}
\end{mdframed}

The output was then sorted using:

\begin{lstlisting}{language=bash}
cat dump.txt | sort | less
\end{lstlisting}


Code was executed using both interactively by logging into a compute node using \texttt{qlogin} and as a batch job using \texttt{qsub}. Short runs of a few minutes were performed interactively when failures occurred quickly or to test a new section of code on smaller data sets. Longer runs that needed more than eight nodes were submitted to the Sun Grid Engine batch system.  

\texttt{valgrind} was used to detect memory leaks and memory management issues. The code was compiled with the debug option \texttt{-ggdb} in order to include symbols. This tool was especially valuable as it could be run using the \texttt{mpirun} command.

\begin{verbatim}
==39048== Mismatched free() / delete / delete []
==39048==    at 0x4C2BB78: realloc (vg_replace_malloc.c:785)
==39048==    by 0x40B056: swapArrayParts(float**, int*, int*, int, int, int*, 
             int, int, ompi_communicator_t*) (swapArrayParts.cpp:115)
==39048==    by 0x407A05: parallelSort(int, int, float**, int*, int*, int, 
             ompi_communicator_t*) (parallelSort.cpp:323)
==39048==    by 0x4066CB: buildTree_parallel(float**, int*, int, Tree*, 
             ompi_communicator_t*, int, int) (buildTree_parallel.cpp:61)
==39048==    by 0x406034: buildTree(float**, int*, int, Tree*, 
             ompi_communicator_t*, int, int, std::string) (buildTree.cpp:42)
==39048==    by 0x402D82: main (main.cpp:177)
==39048==  Address 0x8825ed0 is 0 bytes inside a block of size 200,032 alloc'd
==39048==    at 0x4C2A8A8: operator new[](unsigned long) (vg_replace_malloc.c:423)
==39048==    by 0x402BD2: main (main.cpp:146)
\end{verbatim}

At the end of execution \texttt{valgrind} provides a summary of issues including memory leakage.

\begin{verbatim}
==39047== ERROR SUMMARY: 2 errors from 2 contexts (suppressed: 0 from 0)
==39045== LEAK SUMMARY:
==39045==    definitely lost: 237,355 bytes in 82 blocks
==39045==    indirectly lost: 3,093,358 bytes in 25,667 blocks
==39045==      possibly lost: 320,000,000 bytes in 1 blocks
==39045==    still reachable: 156,179 bytes in 226 blocks
==39045==         suppressed: 0 bytes in 0 blocks
\end{verbatim}

\texttt{gdb} was useful for identifying segmentation fault causes but required the code to be executed on a single core without MPI.

Jobs were submitted to the Sun Grid Engine using \texttt{qsub} and a qsub file. The script used was:

\begin{minipage}{\linewidth}
\lstset{language=bash, keepspaces=true}
\begin{lstlisting}
#!/bin/bash
##
#$ -N MetropolisKdTree64
#$ -pe mpi 64
#$ -q amd8.q
#$ -cwd

mpirun -np $NSLOTS $HOME/COMS7900kdTree/code/kdTree/parallelApproved/main
\end{lstlisting}
\end{minipage}

One improvement that would have made debugging and performance collection significantly easier would be to pass controlling variables as command line parameters. The two key variables are the number of rows to read from each file and the number of search points; these were controlled by constants in the code and required recompilation to test different settings.

Source code was hosted on Github, and each team member had a personal branch for working. Pulls were performed irregularly as members needed to publish significant changes.


%%%%%%%%%
%
% Subsection: main
%
%%%%%%%%%

\subsection{\texttt{main}}

Our \texttt{main} program is divided into three main phases (each of which is comprised of multiple functions): data import, tree building, and tree searching. The data import phase collects all of the relevant filenames of the data files, reads them, and places them into a single 1D array. The tree building phase begins by initializing an empty \texttt{Tree} \texttt{struct} (see below for explanation of the \texttt{struct}) which is then passed into the \texttt{buildTree} function which alters the \texttt{struct}'s contents. The entire tree can then be navigated by using the tree's left, right, and parent fields. The final tree searching phase takes the completed tree \texttt{struct} as an argument along with a search sphere. The number of points within the search sphere is calculated for a list of search spheres and radii.

There are four variables which govern the performance of the $k$-d tree and the size of the problem. First and second are the number of data files and the number of lines per file to read. These values are set in the beginning of \texttt{main} and they have a maximum value of 500 and 20,000,000, respectively. Third, is the number of compute nodes. In general, increasing this number let's us increase the problem size. Its value is set in the command line when running the program and its value must be smaller than the number of files. Last, is the number of lines to read from the 501-st data file. These are the centers of the search spheres. The value is set in \texttt{search501}.


%%%%%%%%%
%
% Subsection: listFiles
%
%%%%%%%%%

\subsubsection{\texttt{listFiles}}\label{sec:listfiles}

The \texttt{listFiles} function has the following definition:

\lstset{language=C++, keepspaces=true}
\begin{lstlisting}
vector<string> listFiles(string path, int numFiles); 
\end{lstlisting}

It accepts two parameters: \\

\begin{tabular}{l l}
\texttt{path} & The path to the data files to be used \\
\texttt{numFiles} & The maximum number of files to be used by the program \\
\end{tabular}\\

The return value is a Standard Template Library \texttt{vector} containing strings with the filenames. The vector has a maximum length of \texttt{numFiles}. Files are not gathered in alphabetical order but in the order returned from the operating system. For example, for \texttt{numFiles = 10}, it is unlikely that the files returned will be \texttt{datafile00001.txt} through \texttt{datafile00010.txt}.


%%%%%%%%%
%
% Subsection: distributeFiles
%
%%%%%%%%%

\subsubsection{\texttt{distributeFiles}}\label{sec:distributefiles}

The \texttt{distributeFiles} function is defined as:

\lstset{language=C++, keepspaces=true}
\begin{lstlisting}
void distributeFiles(vector<string> files, int numWorkers);
\end{lstlisting}

It accepts two parameters: \\

\begin{tabular}{l l}
\texttt{files} & The list of files returned by \texttt{listFiles} \\
\texttt{numWorkers} & The number of nodes allocated for the job \\
\end{tabular} \\

The function does not return any values to the caller. It iterates over the list of files and sends them to the workers in a round-robin method to ensure that the data is distributed as evenly as possible without knowing the true length of each data file. Files are sent asynchronously since rank 0 participates in the construction of the tree and searching.

%%%%%%%%%
%
% Subsection: receiveFiles
%
%%%%%%%%%

\subsubsection{\texttt{receiveFiles}}\label{sec:receivefiles}

The \texttt{receiveFiles} function is defined as:

\lstset{language=C++, keepspaces=true}
\begin{lstlisting}
vector<string> receiveFiles(int myRank);
\end{lstlisting}

It accepts a single parameter: \\

\begin{tabular}{l l}
\texttt{myRank} & The MPI rank of the current process \\
\end{tabular} \\

\texttt{receiveFiles} performs an \texttt{MPI\_Receive} to obtain a list of files sent by \texttt{distributeFiles}. These are stored in an STL \texttt{vector} and returned to the calling function.


%%%%%%%%%
%
% Subsection: importFiles
%
%%%%%%%%%

\subsubsection{\texttt{importFiles}}\label{sec:importFiles}

% import: gets the list of files and imports their data into an array

The \texttt{importFiles} function is defined as:

\begin{minipage}{\linewidth}
\lstset{language=C++, keepspaces=true}
\begin{lstlisting}
void importFiles(vector<string> files, int myRank,
	float *myData, int *rows, int *cols, 
	int maxRowsPerFile, unsigned long int arrayLimit);
\end{lstlisting}
\end{minipage}

This function accepts seven parameters: \\

\begin{tabular}{l l}
\texttt{\texttt{files}}          & An STL vector of files from \texttt{receiveFiles} \\
\texttt{\texttt{myRank}}         & The MPI rank of the current process \\
\texttt{\texttt{myData}}         & A one-dimensional array in which to store the data from the files \\
\texttt{\texttt{rows}}           & A pointer to a variable to store the number of rows imported \\
\texttt{\texttt{cols}}           & A pointer to a variable to store the number of columns imported  \\
\texttt{\texttt{maxRowsPerFile}} & The maximum number of rows to read from each file \\
\texttt{\texttt{arrayLimit}}     & The maximum size of the allocated array \\
\end{tabular} \\

The \texttt{importFiles} function iterates over the list of files received and opens each for reading. It uses \texttt{fscanf} to read each line directly into the array pointed to by \texttt{myData} in order to minimize data movement. The file is closed when the end of the file is reached, \texttt{maxRowsPerFile} is met, or the total data read from all files equals \texttt{arrayLimit}. The number of rows is written to \texttt{rows} and the number of columns to \texttt{cols}. The variable \texttt{myRank} is passed for debugging purposes. Results are returned through the references passed as variables.

A significant improvement would have been to convert the ASCII files to binary as other teams did. Also, since the local sort was verified to work, a presorted binary file for each node could be created allowing the program to bypass the ASCII to binary conversion, merging multiple files, and performing the initial sort. This would allow each node to simply perform an \texttt{fread} the entire array from the file with one command.


%%%%%%%%%
%
% Subsection: CalculateIndex
%
%%%%%%%%%

\subsubsection{\texttt{CalculateIndex}}\label{sec:calculateindex}

The \texttt{CalculateIndex} function is defined as:

\begin{minipage}{\linewidth}
\lstset{language=C++, keepspaces=true}
\begin{lstlisting}
float CalculateIndex(string filename)
\end{lstlisting}
\end{minipage}

It accepts a single parameter \texttt{filename} which is the file being imported by \texttt{importFiles}. This function parses the numeric value to calculate the starting index for the rows:

\begin{center}
    I$_0$ = (\texttt{fileId} - 1) * \texttt{\_MAX\_ROWS\_}
\end{center}

This avoids the need to handle the integer overflow issue in the later data files. The value is stored as a \texttt{float} to allow a single array to hold both the coordinates from the file and the index.

% build: we build the tree using ORB to partition the data

% search: import the 501-st data file and perform a search on each, summing the total number of points found over several radii


%%%%%%%%%
%
% Subsection: K-d Tree
%
%%%%%%%%%

\subsection{$k$-d tree}
%
% JJ, talk about the Tree \texttt{struct} and what all fields it has
%

The design of the $k$-d tree required storing information for organizing both the parallel distribution of the data and the data held locally on the node. Rather than create two different type of \texttt{struct}ures, the code utilized a single C \texttt{struct}. This \texttt{struct} underwent a massive overhaul/trimming near the end of the project in an attempt to decrease to total memory usage. Initially, the \texttt{struct} was defined as:

\begin{minipage}{\linewidth}
\lstset{language=C++, keepspaces=true}
\begin{lstlisting}
struct Tree {
    // Pointers used for local tree
	Tree *p;    // Parent
	Tree *l;    // Left child
	Tree *r;    // Right child
	int i;      // Sort dimension used to split this node
	int source; // Which buildTree function created it

    // Pointers used for parallel tree
	MPI_Comm parentComm;  // Parent communicator
	MPI_Comm leftComm;    // Left child communicator
	MPI_Comm rightComm;   // Right child communicator
	MPI_Comm thisComm;    // Communicator that the node belongs to

    // Each node in the tree has a unique identifier
	string name;

    // For nodes with children, this holds the min and max for
    // the child nodes
	double x1;  // Min x
	double x2;  // Max x
	double y1;  // Min y
	double y2;  // Max y
	double z1;  // Min z
	double z2;  // Max z

	int depth;  // Depth of the node in the tree
	int n;      // Number of points

	double c[4];    // Center of this tree
	double radius;  // Radius of this tree

	double d[4];   // Data point coordinates
	double index;  // File row index from source data
}
\end{lstlisting}
\end{minipage}


This \texttt{struct} contains multiple fields which either were only used for debugging or not even used at all. Clearly, this \texttt{struct}ure proved to be too bloated when attempting to import all 10 billion rows of data as well as the 20 million rows of search rows. The so-called ``trimmed" tree \texttt{struct} is defined as:

\begin{minipage}{\linewidth}
\lstset{language=C++, keepspaces=true}
\begin{lstlisting}
struct Tree {
	Tree *p;  // Parent
	Tree *l;  // Left child
	Tree *r;  // Right child

	MPI_Comm parentComm, leftComm, rightComm, thisComm;

	float x1;  // Min x
	float x2;  // Max x
	float y1;  // Min y
	float y2;  // Max y
	float z1;  // Min z
	float z2;  // Max z

	float c[4];  // Center of this tree
	float radius;

	float d[4];  // Data point
}
\end{lstlisting}
\end{minipage}

As can be seen, we made an additional change by changing all \texttt{double} data types to \texttt{float} types in order to conserve memory. This change affected every portion of the code, so we performed a global find and replace on all \texttt{.cpp, .h} files in our repository using the following:

\lstset{language=C++, keepspaces=true}
\begin{lstlisting}
	find ./ -type f -exec sed -i 's/DOUBLE/FLOAT/g' {} \;
	find ./ -type f -exec sed -i 's/double/float/g' {} \;
\end{lstlisting}

This change only required us to alter the \texttt{fscanf} statement to read \texttt{float}'s as opposed to \texttt{long float}'s.

During the parallel construction phase, nodes are separated into different MPI communicators as shown in Figure~\ref{fig:communicators}. The four \texttt{MPI\_Comm} variables store the references to the parent, left and right children, and current communicators. The value \texttt{MPI\_COMM\_SELF} is used as a ``null'' value for these pointers to indicate no communicators exist in the given direction.

Once the nodes became members of a communicator containing only itself, \texttt{buildTree\_serial} constructed the local serial tree using the same \texttt{struct}ure but using the \texttt{Tree} pointers rather than the \texttt{MPI\_Comm} variables.


%%%%%%%%%
%
% Subsubsection: Constants
%
%%%%%%%%%

\subsubsection{Constants}\label{sec:constants}

C preprocessor identifiers were defined and used in place of numeric constants for readability. Data points were defined as an array of \texttt{float}'s of size four consistently even when only three values were needed. The four positions were referenced using:

\lstset{language=C++, keepspaces=true}
\begin{lstlisting}
#define _INDEX_   0  // Row index from source datafile
#define _X_       1
#define _Y_       2
#define _Z_       3
\end{lstlisting}

A single tree is created and populated by both the parallel and serial versions of \texttt{buildTree}. To aid with debugging, each node is marked with an identifier to show which function created it. The identifiers for the sources are:

\lstset{language=C++, keepspaces=true}
\begin{lstlisting}
#define _Source_buildTree_unknown     0
#define _Source_buildTree_parallel   -1
#define _Source_buildTree_serial     -2
\end{lstlisting}

%%%%%%%%%
%
% Subsubsection: Naming
%
%%%%%%%%%

\subsubsection{Tree Node Naming}

Each node in the tree is assigned a unique identifier determined by its position. The top node is named \texttt{t}. The two child nodes of \texttt{t} are \texttt{tl} and \texttt{tr}. An \texttt{l} or \texttt{r} is appended to the parent's name to designate the direction it is below the parent. The final node of the parallel section appends an \texttt{*} to designate the transition from parallel to serial. The node of the tree containing the data point appends an \texttt{!}. This was one of the fields which was trimmed away after debugging was completed.

%%%%%%%%%
%
% Subsubsection: Parallel Variable
%
%%%%%%%%%

\subsubsection{Parallel variables}

The \texttt{struct} variables used during the initial division of the tree among the nodes and their purpose are: \\

\begin{tabular}{l l}
\texttt{i} & The dimension that was split by the MPI communicator.  \\
\texttt{source} & During the parallel phase, this value is set to  \\
& \texttt{\_Source\_buildTree\_parallel}.  \\
\texttt{parentComm} & The parent MPI communicator that created this node. The\\
&  root node stores a value of \texttt{MPI\_COMM\_SELF} in lieu of a null. \\
\texttt{leftComm} & The MPI communicator of the \textit{left} child tree for the current compute node.  \\
\texttt{rightComm} & The MPI communicator of the \textit{right} child tree for the current compute node.  \\
\texttt{thisComm} & The MPI communicator of the current compute node.  \\
texttt{x1} & The minimum value of \textit{x} stored in this subtree.  \\
\texttt{x2} & The maximum value of \textit{x} stored in this subtree.  \\
\texttt{y1} & The minimum value of \textit{y} stored in this subtree.  \\
\texttt{y2} & The maximum value of \textit{y} stored in this subtree.  \\
\texttt{z1} & The minimum value of \textit{z} stored in this subtree.  \\
\texttt{z2} & The maximum value of \textit{z} stored in this subtree.  \\
\texttt{depth} & The depth within the tree of the current node. The top node has a \\
& depth of zero.  \\
\texttt{n} & The number of points contained within the subtree where the current \\
& node is the root.  \\
\texttt{c[4]} & The center of the subtree. Index for the array is described in section \ref{sec:constants}. \\
\texttt{radius} & The distance from the center of the subtree to the furthest point.  \\
\end{tabular} \\

\begin{center}
   \begin{figure}[b!]
      \includegraphics[width=0.9\textwidth]{images/communicators.png}
      \caption{Example of parallel variables using eight nodes}
      \label{fig:communicators}
   \end{figure}
\end{center}

%%%%%%%%%
%
% Subsubsection: Serial variables
%
%%%%%%%%%

\subsubsection{Serial variables}

The \texttt{struct} variables used during the final division of the local serial tree on the compute nodes and their purpose are: \\

\begin{tabular}{l l}
\texttt{p} & Pointer to the parent of the current node. This value is \texttt{null} for the top node. \\
\texttt{l} & Pointer to the \textit{left} child of the current node. This value is \texttt{null} for the data point. \\
\texttt{r} & Pointer to the \textit{right} child of the current node. This value is \texttt{null} for the data point. \\
\texttt{i} & The dimension that was split during the local tree construction. \\
\texttt{source} & During the serial phase, this value is set to \texttt{\_Source\_buildTree\_serial}. \\
\texttt{x1} & The minimum value of \textit{x} stored in this subtree. \\
\texttt{x2} & The maximum value of \textit{x} stored in this subtree. \\
\texttt{y1} & The minimum value of \textit{y} stored in this subtree. \\
\texttt{y2} & The maximum value of \textit{y} stored in this subtree. \\
\texttt{z1} & The minimum value of \textit{z} stored in this subtree. \\
\texttt{z2} & The maximum value of \textit{z} stored in this subtree. \\
\texttt{depth} & The depth within the tree of the current node. The top node has a depth of zero. \\
\texttt{n} & The number of points contained within the subtree where the current node \\
& is the root. \\
\texttt{c[4]} & The center of the subtree. Index for the array is described in section \ref{sec:constants}. \\
\texttt{radius} & The distance from the center of the subtree to the furthest point. \\
\end{tabular} \\


%%%%%%%%%
%
% Subsubsection: Data point variables
%
%%%%%%%%%

\subsubsection{Data point variables}

The \texttt{struct} variables used for storing the data point within the tree and their purpose are:

\begin{tabular}{l l}
 \texttt{p} & Pointer to the parent of the current node.  \\
 \texttt{l} & Pointer to the \textit{left} child of the current node. This value is \texttt{null} for the data point.  \\
 \texttt{r} & Pointer to the \textit{right} child of the current node. This value is \texttt{null} for the data point.  \\
 \texttt{source} & During the serial phase, this value is set to \texttt{\_Source\_buildTree\_serial}.  \\
 \texttt{depth} & The depth within the tree of the current node. The top node has a depth of zero.  \\
 \texttt{n} & The number of points contained within the subtree where the current node is the root.  \\
 \texttt{d[4]} & The data point.  \\
 \texttt{index} & The row index of the data point as read from the source file.  \\
 \end{tabular}


%%%%%%%%%
%
% Subsection: Building the tree
%
%%%%%%%%%

\subsubsection{Building the tree}
% General notes on the method...

\paragraph{\texttt{buildTree}}
% farzi, just add the listing
This function gets the number of compute nodes $q$ available in the current communicator and determines which function to run. If $q>1$, then we can still do a parallel sort with at least two compute nodes, so \texttt{buildTree\_parallel} is entered. If $q=1$, then \texttt{buildTree\_serial} is entered.


%%%%%%%%%
%
% Function: buildTree_serial
%
%%%%%%%%%

\paragraph{\texttt{buildTree\_serial}}
This was the first function written after our initial prototyping phase was completed. It essentially performs a serial version of ORB which can be executed on a single compute node.

%
% JJ, you wanna write this since you wrote the program?
%

Upon completion, \texttt{buildTree\_serial} recursively calls itself instead of \texttt{buildTree} since we still have $q=1$.


\paragraph{\texttt{buildTree\_parallel}}
This function performs essentially the same tasks as \texttt{buildTree\_serial}, but utilizing multiple nodes. Specifically, it takes advantage of \texttt{parallelSort} in order to partition the data along the longest axis (determine by \texttt{getSortDim}, discussed below). After the data has been sorted, the data is split by placing lower half (w.r.t. the sorted data values) of the compute nodes into a left communicator and the upper half into a right communicator. Each half then calls \texttt{buildTree}.

% Upon completion, \texttt{buildTree\_parallel} has spawned 2 new communicators (a left and a right) each of which has half as many compute nodes as the parent communicator. All of the nodes from each side then call \texttt{buildTree} so that they can perform the $q$ check.


\paragraph{\texttt{getSortDim}}
This function is used by \texttt{buildTree\_parallel} in order to determine which is the longest axis, i.e., the sort dimension. In addition to this, it also fills in the \texttt{struct} for the current tree node with the global min/max in all three dimensions. To do this, each node determines its own min/max for each axis and sends them to rank 0 (w.r.t. the current communicator). Rank 0 then determines the global min/max and then MPI\_Bcast's those values back to the other ranks. This allows each communicator to sort their data independently along different axes. The center of the bounding box defined by these values min's/max's is also stored in the tree \texttt{struct}. 


\subsubsection{Searching the tree}



\paragraph{\texttt{searchTree\_serial}}
% GW
This function is called by all ranks and returns an integer which equals the number of points found by the rank that called it. It takes as arguments the point and radius (which specifies a search sphere) and the root of the tree created by \texttt{buildTree}. To perform the search, a check is done to determine if the search sphere intersects the bounding sphere of each data partition:
\begin{equation}
		r^2_\textrm{sphere center to box center} \le (r_{sphere} + r_{box})^2
\end{equation}
We used the squared version of the formula to avoid computing the square root, thus saving time. If this check returns false, then there are no data points within the search sphere contained in that partition and the function exits. If true, then another check is performed to check if the left or right child trees are NULL. If they are both NULL, then by our definition of the tree, we have found a point that is a leaf of the tree, thus we increment the count and exit the function. Otherwise, we recursively call the function again using the correct child tree.

%%%%%%%%%
%
% Function: search501
%
%%%%%%%%%

\paragraph{\texttt{search501}}
%
% JJ, edit if you want since you wrote most of this
%

Each compute node calls this function which reads the data file \\
\texttt{datafile00501.txt} whose contents are the centers of the search spheres. The function uses \texttt{importFiles} to read the file and passes the number of search rows in \texttt{maxRowsPerFile}. It then loops \texttt{searchTree\_serial} for all of the sphere centers for three different radii (0.01, 0.05, 0.10). The counts on each compute node are then stored. After all searches are complete, an \texttt{MPI\_Reduce} is performed by all nodes, thus adding all of the counts into a single array which is sent to \texttt{stdout}. When the job is submitted using \texttt{qsub}, the output is directed to a file with the name of the job. We did not make the final transition from local node searches to a parallel search due to time and computing constraints.

\subsection{Altered parallel sorting}
We had to adjust our parallel sorting algorithm a great extend to integrate it into the new project.

\subsubsection{\texttt{parallelSort}}

% GW, farzi

\paragraph{Conversion to function}
One of our first obstacles was modifying our original \\
\texttt{parallelSort} program to work for this project. The first issue was having \texttt{parallelSort} operate as a function. We accomplished this by removing the MPI setup and Data Import sections from \texttt{parallelSort} and placing them into a new \texttt{main}. While doing this we encountered issues with how the data was passed using both a pointer and array notation. Because several functions within the \texttt{parallelSort} function would need to change both the number of rows and the data we had to pass as pointers and pointers to arrays. 


\paragraph{Making rank 0 do work}
The second issue was our use of rank 0 as the manager in our original \texttt{parallelSort} program. Our implementation of \texttt{parallelSort} had rank 0 manage while all the other ranks were contained and performed operations on the actual data. However, when utilizing a $k$-d tree, all of the ranks must assist in the workload, otherwise each time the tree forked and a new communication branch was formed, the process would lose a rank due to a new rank 0 being formed. To resolve this problem we had to make several clever adjustments in the sections of the code, most notably the adaptive binning section. Other sections just needed their initial iteration set to 0 instead of 1 or the conditional of an \texttt{if} statement changed.

\paragraph{Using different communicators}
Since parallel ORB requires multiple communicators, it was necessary that \texttt{parallelSort} (any many functions it contains) was given the current communicator as an argument. This was not a difficult modification, but it was quite tedious since there were many functions which required alterations to their header files.

\subsubsection{\texttt{adaptBins}}
Although our original adaptive binning scheme performed relatively well, we wanted something even better since ORB requires many \texttt{parallelSort} calls. As a review, here is our original method:
\begin{equation}
	\begin{split}
		\Delta C & = 2.0 ( C^{m}_{i+1} - C^{m}_i ) / ( C^{m}_{i+1} + C^{m}_i ) \\
		\Delta E & = E^m_{i+1} - E^m_i \\
		E^{m+1}_i & = E^m_i + \alpha \Delta C \Delta E
	\end{split}
\end{equation}
% added scale factor
where $C$ are the bin counts, $E$ are the bin edges, and $\alpha < 0.5$. This method will occasionally devolve into oscillatory behavior and not converge to the correct value. To combat this in the ORB project, we added a scale factor $S$ which decreases over time:
\begin{equation}
	\begin{split}
		S(m) & = 1 - (1 - 0.1) (1 - \textrm{exp}(-0.03 m) \\
		E^{m+1}_i & = E^m_i + \alpha S(m) \Delta C \Delta E
	\end{split}
\end{equation}

Since this method is local, it converges slowly at bin edges far away from high-density clusters. As such, we considered replacing this method with a global method which uses linear interpolation to estimate where the bins would be evenly distributed. Define the function $\hat C(x)$ as the linear approximation of the cumulative count distribution of the data points (so that it normalizes to the number of data points, not unity). Then,
\begin{equation}
		\hat C(x) = \hat C(E^m_{i'}) + C^m_{i'} \dfrac{x - E^m_{i'}}{E^m_{{i'}+1} - E^m_{i'}} = (i+1) \dfrac{D}{N}
\end{equation}
where $i'$ is the maximum index such that $\hat C(E_{i'}) < (i+1) \dfrac{D}{N}$ (note that $i'$ and $i$ are distinct integers). The right equality implies that we should solve for the value of $x$ such that it holds. This $x$ value will be the new $i$-th bin edge, $E^{m+1}_i$. Therefore,
\begin{equation}
		E^{m+1}_i = E^m_{i'} + \Big( (i+1) \dfrac{D}{N} - C(E^m_{i'}) \Big) (E^m_{{i'}+1} - E^m_{i'}) / C^m_{i'}
\end{equation}

We discovered that this adaptation technique performs incredibly well, but is also prone to oscillations near clusters of points (but to a much lower frequency than the earlier method). In order to resolve this oscillation issue, are solution was to simply alternate between the two methods at each step. This solved all of our convergence issues. (Note that we still use the same binary search-based binning technique and stopping criterion from the previous project.)

%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
\section{Validation}

% GW, farzi
\subsection{Two MATLAB demos}

\subsubsection{2D animation of tree building}
When prototyping the code in MATLAB, we generated animated visualizations of the construction of a 2-d tree. Due to the format, we cannot include the animation within this paper, but it shall be included in the presentation.

\subsubsection{3D tree partitions from different orientations}
During our development of the code we wanted a way to verify that ORB was being performed correctly. Therefore, after the parallel phase of the $k$-d tree was completed, we output the $x,y,z$ min's/max's of the each partition in the tree and saved the values to a file. We then wrote a MATLAB script that would plot boxes defined these values. Figure \ref{square} provides an example of the type of image this script produced. It depicts the boundaries of 25 partitions. As can be seen from this image, each of the bounding boxes vary in limiting size, however all the boxes contain a relatively equal number of points.

\begin{figure}[b!]
	\includegraphics[width=0.9\textwidth]{images/squares.png}
	\caption{Example $k$-d tree at 25 partitions.}
	\label{square}
\end{figure}

\subsection{Other}
Another testing method we performed during our development was validation through variation of input parameters. We held constant the number of data points read, the searching point and search radius, but varied the number of nodes that were used in the process. Then we had each node output the number of data points within their tree that fell within the search point and radius. Although the number of points per tree would vary depending upon the number of nodes used, the sum of all those points would remain constant. From this test we confirmed that the search tree was working as intended. 

We also varied the search radius in three ways. First, we made the search radius very large and---as expected---we found all points. Second, we the search radius to a very small number and the search point to a point within the data files. As expected, only one points was found. Third, we set the search point and ran several tests with increasing radii. As expected, the number of points found increased monotonically with the radius.

These validation techniques make it clear to within a reasonable doubt that our code functions as intended.

%%%%%%%%%
%
% Section: Results
%
%%%%%%%%%

\section{Results}

% JJ

\input{results.tex}


%%%%%%%%%%%%%
%%%
%%% Conclusions
%%%
%%%%%%%%%%%%%

\section{Conclusions}
% everyone
This project was an excellent test for managing MPI in a memory-restricted environment.
%the context of large amounts of data and long run times.
It also provided an opportunity to work as a well-functioning team and develop a synergistic group dynamic. Together, we were able to overcome significant setbacks. Below is a summary of different aspects of our performance, including some challenges which we struggled with as well as our successes and possible future work.

\medskip

\begin{minipage}{\linewidth}
\begin{mdframed}[backgroundcolor=red!20]
	Challenges:
	\begin{itemize}
		\item memory management (leaks, limited space, Signal 9)
		\item array out of bounds issues
		\item multiple communicators (comm)
		\item no planning for function arguments and return values (constant editing of h-files)
		\item testing was difficult due to cluster overloading and hardware errors
		\item debug print statement clutter
		\item inconsistent usage pointer-to-pointer calls for *data[] and *rows (due to \texttt{swapArrayParts})
		\item \texttt{malloc} when you should \texttt{realloc}
		\item \texttt{parallelSort} conversions
		\item \texttt{adaptBins} convergence problems
	\end{itemize}
\end{mdframed}
\end{minipage}

\medskip

\begin{minipage}{\linewidth}
\begin{mdframed}[backgroundcolor=green!20]
	Successes:
	\begin{itemize}
		\item few merge conflicts and fast coding through extreme coding and Git branches
		\item efficient delegation of tasks
		\item visualizing output through MATLAB
		\item excellent validation techniques
	\end{itemize}
\end{mdframed}
\end{minipage}

\medskip

\begin{minipage}{\linewidth}
\begin{mdframed}[backgroundcolor=blue!20]
	Future work:
	\begin{itemize}
		\item cloud computing
		\item use of coding techniques for personal research
	\end{itemize}
\end{mdframed}
\end{minipage}

\medskip

%%%%%%%%%%%%%%%
%
% Recommendations
%
%%%%%%%%%%%%%%%

\subsection{Recommendations}

The following recommendations could improve the course when offered next:

\begin{itemize}
    \item Assign a job queue to each team with dedicated nodes for each to eliminate resource conflicts between teams.
    \item Spend one lecture on using Git and Github
    \item Provide a ``big picture'' view of how the first assignment of parallel sort will be used in future projects
    \item Break the projects into smaller assignments:
        \begin{itemize}
            \item Initial assignment is simply having an MPI program that distributes files evenly across nodes and reads the data in
            \item Parallel sort assignment
            \item Build local $k$-d tree on each node and have each node read and search the 501 file
            \item Assignment to organize nodes into groups of MPI communicators and pass data through the tree
            \item Final project is parallel $k$-d tree
        \end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%
%
% Links
%
%%%%%%%%%%%%%%%

\subsection{Links}

\begin{verbatim}
* k-d tree repo
  https://github.com/jjlay/COMS7900kdTree

* Production source
  https://github.com/jjlay/COMS7900kdTree/tree/master/code/kdTree/parallelApproved

* Paper
  https://github.com/jjlay/COMS7900kdTree/tree/master/paper

* Presentation
  https://github.com/jjlay/COMS7900kdTree/tree/master/presentation
  
* ParallelSort
  https://github.com/jjlay/COMS7900kdTree/tree/master/code/parallelSort
\end{verbatim}

\subsection{Contributions}

It is very difficult to state that any one team member was the sole contributor to a section of the code. After the parallel sort project, we adopted an ``extreme coding'' approach during which all three team members sat in front of a single console while one person typed. Almost all of the $k$-d tree code was developed this way. Rather than identify a single person as the developer, it is more appropriate to identify a ``technical lead'' for each section. \\

\begin{tabular}{l l c}
\textbf{Function}  & \textbf{Lead Member} & \textbf{Magnitude of Work} \\
GitHub guru & JJ Lay & 1 \\
listFiles & JJ Lay & 2 \\
distributeFiles & JJ Lay & 2 \\
receiveFiles & JJ Lay & 2 \\
importFiles & JJ Lay & 4 \\
buildTree\_serial & JJ Lay & 4 \\
searchTree\_serial & JJ Lay & 6 \\
search501 & JJ Lay & 4 \\
localSort & James Farzidayeri & 6 \\
swapArrays & James Farzidayeri & 10 \\
cleanUp & James Farzidayeri & 9 \\
binData & Graham West & 5 \\
adaptBins & Graham West & 8 \\
uniformity & Graham West & 2 \\
Paper & Graham West & 5 \\
Presentation & Graham West & 5 \\
parallelSort & Everyone & 10 \\
buildTree\_parallel & Everyone & 10 \\
\end{tabular} \\

%%%%%%%%%%%%%
%%% NEW SECTION %%%
%%%%%%%%%%%%%
% \section{Bibliography}

\end{document}
